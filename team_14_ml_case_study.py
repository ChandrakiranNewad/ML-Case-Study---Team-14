# -*- coding: utf-8 -*-
"""Team_14_ML_Case_Study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fi1i20MWushejr1nlEHDwweJx9D-O8s_
"""

# importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score

# uploading data

lead_df = pd.read_csv("Leads.csv")

#checking no of records and no of columns

lead_df.shape

lead_df.info()

# checking first five rows

lead_df.head()

# creating a copy of initial data

lead_df2 = lead_df

#Dropping Prospect ID and Lead Number columns as they are just working as index
#There will no be any significance in output of prediciton of this two columns

lead_df = lead_df.drop(['Prospect ID','Lead Number'],axis=1)

lead_df.shape

# checking null values in each column

lead_df.isna().sum()

"""* 17 columns are having null values."""

# percentage of null values in each column

print("Percentage of null values:\n")
for i in lead_df.columns:
  if(lead_df[i].isnull().sum() != 0):
    print(i+":", round((lead_df[i].isnull().sum()/lead_df.shape[0]) * 100, 2))

"""* Some columns have null values more than 40%.
* It is better to drop these columns instead of filling them.
"""

# dropping columns based on null value percentage

lead_df = lead_df.drop(['Lead Quality','Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'],axis=1)

lead_df.shape

"""## Analyzing each categorical feature"""

# checking unique values

lead_df['Lead Source'].unique()

# count plot of sources

plt.figure(figsize = (9, 7))
sns.countplot(y = lead_df["Lead Source"].values, order = lead_df["Lead Source"].value_counts().index)
plt.title("Count plot of Sources")
plt.show()

"""* From the graph it is evident that Google is the major lead source"""

# filling null values

lead_df['Lead Source'] = lead_df['Lead Source'].fillna('Google')

# checking unique values

lead_df['Country'].unique()

# count plot of Country

plt.figure(figsize = (9, 10))
sns.countplot(y = lead_df["Country"].values, order = lead_df["Country"].value_counts().index)
plt.title("Count plot of Country")
plt.show()

# checking unique values

lead_df["City"].unique()

# getting number of records where City column has Indian cities, but respective Country value is missing

lead_df[lead_df["City"].isin(["Mumbai", "Thane & Outskirts", "Other Cities of Maharashtra"])]["Country"].isnull().sum()

# filling Country column with "India" where there are Indian cities in City column

for i in ["Mumbai", "Thane & Outskirts", "Other Cities of Maharashtra"]:
  lead_df.loc[lead_df.City == i, "Country"] = "India"

# some records have Country as unknown, changing them to maintain consistency

lead_df.loc[lead_df.Country == "unknown", "Country"] = "Unknown"

# filling the left null values with Unknown

lead_df["Country"] = lead_df["Country"].fillna("Unknown")

# checking unique values

lead_df['Specialization'].unique()

# count plot of Specialization

plt.figure(figsize = (8, 7))
sns.countplot(y = lead_df["Specialization"].values, order = lead_df["Specialization"].value_counts().index)
plt.title("Count plot of Specialization")
plt.show()

# Select is invalid option and in this case it implies the value is missing. So, converting these values into null

lead_df['Specialization'] = lead_df['Specialization'].replace('Select',np.nan)

# filling null values as Unknown

lead_df['Specialization'] = lead_df['Specialization'].fillna('Unknown')

# checking unique values

lead_df['What is your current occupation'].unique()

# count plot of What is your current occupation

plt.figure(figsize = (8, 5))
sns.countplot(y = lead_df['What is your current occupation'].values, order = lead_df['What is your current occupation'].value_counts().index)
plt.title('Count plot of What is your current occupation')
plt.show()

"""* Since it is a categorical column, we can replace null values by the mode of that column."""

# filling null values using mode

lead_df['What is your current occupation'] = lead_df['What is your current occupation'].fillna(lead_df['What is your current occupation'].mode()[0])

# checking unique values

lead_df['Do Not Email'].unique()

# count plot of Do Not Email

sns.countplot(x=lead_df['Do Not Email'].values)
plt.title("Count plot of Do Not Email")
plt.show()

# checking unique values

lead_df['Do Not Call'].unique()

sns.countplot(x = lead_df['Do Not Call'].values)
plt.title("Count plot of Do Not Call")
plt.show()

# checking unique values

lead_df['Last Activity'].unique()

# count plot of Last Acitivity

plt.figure(figsize = (9, 7))
sns.countplot(y = lead_df['Last Activity'].values, order = lead_df['Last Activity'].value_counts().index)
plt.title('Count plot of Last Activity')
plt.show()

"""* From this it is clear that we can replace null values with mode of the column."""

# filling null values with the mode

lead_df['Last Activity'] = lead_df['Last Activity'].fillna(lead_df['Last Activity'].mode()[0])

# checking unique values

lead_df['How did you hear about X Education'].unique()

# count plot of How did you hear about X Education

plt.figure(figsize = (8, 5))
sns.countplot(y = lead_df['How did you hear about X Education'].values, order = lead_df['How did you hear about X Education'].value_counts().index)
plt.title('Count plot of How did you hear about X Education')
plt.show()

"""* Select is invalid, we have to replace it."""

# replacing select as null values

lead_df['How did you hear about X Education'] = lead_df['How did you hear about X Education'].replace('Select',np.nan)

plt.figure(figsize = (8, 5))
sns.countplot(y = lead_df['How did you hear about X Education'].values, order = lead_df['How did you hear about X Education'].value_counts().index)
plt.title('Count plot of How did you hear about X Education')
plt.show()

# checking unique values

lead_df['What matters most to you in choosing a course'].unique()

# count plot of What matters most to you in choosing a course

plt.figure(figsize = (8, 5))
sns.countplot(x = lead_df['What matters most to you in choosing a course'].values, order = lead_df['What matters most to you in choosing a course'].value_counts().index)
plt.title('Count plot of What matters most to you in choosing a course')
plt.show()

"""* Majority of the column has Better Career Prospects as it's value.
* So, we can replace null value with Better Career Prospects.
"""

# filling null values with most occurred value

lead_df['What matters most to you in choosing a course'] = lead_df['What matters most to you in choosing a course'].fillna('Better Career Prospects')

# checking unique values

lead_df['Search'].unique()

sns.countplot(x = lead_df['Search'].values)
plt.title("Count plot of Search")
plt.show()

# checking unique values

lead_df['Magazine'].unique()

sns.countplot(x = lead_df['Magazine'].values)
plt.title("Count plot of Magazine")
plt.show()

# checking unique values

lead_df['Newspaper'].unique()

sns.countplot(x = lead_df['Newspaper'].values)
plt.title("Count plot of Newspaper")
plt.show()

# checking unique values

lead_df['Tags'].unique()

plt.figure(figsize = (9, 7))
sns.countplot(y = lead_df['Tags'].values, order = lead_df["Tags"].value_counts().index)
plt.title("Count plot of Tags")
plt.show()

# filling null values with mode

lead_df['Tags'] = lead_df['Tags'].fillna(lead_df['Tags'].mode()[0])

# checking unique values

lead_df['Lead Profile'].unique()

plt.figure(figsize = (15, 7))
sns.countplot(x = lead_df['Lead Profile'].values, order = lead_df["Lead Profile"].value_counts().index)
plt.title("Count plot of Lead Profile")
plt.show()

lead_df['Lead Profile'] = lead_df['Lead Profile'].replace('Select',np.nan)

# checking unique values

lead_df['City'].unique()

# count plot of City

sns.countplot(y = lead_df['City'].values, order = lead_df['City'].value_counts().index)
plt.title('Count plot of City')
plt.show()

"""* We will replace Select as null value.
* Then we will replace null value with mode of the column.
"""

# replacing Select as null

lead_df['City']=lead_df['City'].replace('Select',np.nan)

# checking records where Country is India and the respective City value is null

lead_df[lead_df['Country'].isin(['India'])]['City'].isnull().sum()

# replacing null values with Mumbai where Country is India

lead_df.loc[lead_df.Country == 'India', 'City'] = 'Mumbai'

lead_df[lead_df['Country'].isin(['India'])]['City'].isnull().sum()

# filling the other null values in City column with Unknown

lead_df['City'] = lead_df['City'].fillna('Unknown')

lead_df.isna().sum()

# From above we can drop column which are having more than 50% null values

lead_df = lead_df.drop(['How did you hear about X Education','Lead Profile'],axis=1)

"""## Checking biased categorical data
* Till now we have replaced missing values and null values.
* But from observation we have found that we have imbalnaced data in our dataset.
* We will need to clear the imbalanced feature data.
* We are taking a call that if data is biased by 70% then we are going to drop that column.
"""

lead_df.info()

"""* In the following code cells we will be calculating distribution of different values in the said column."""

(lead_df['Lead Origin'].value_counts(normalize = True)) * 100

(lead_df['Lead Source'].value_counts(normalize = True)) * 100

(lead_df['Do Not Email'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Do Not Call'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Last Activity'].value_counts(normalize = True)) * 100

(lead_df['Country'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Specialization'].value_counts(normalize = True)) * 100

(lead_df['What is your current occupation'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['What matters most to you in choosing a course'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Search'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Magazine'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Newspaper Article'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['X Education Forums'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Newspaper'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Digital Advertisement'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Through Recommendations'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Receive More Updates About Our Courses'].value_counts(normalize = True)) * 100
# need to  drop

(lead_df['Tags'].value_counts(normalize = True)) * 100

(lead_df['Update me on Supply Chain Content'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['Get updates on DM Content'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['City'].value_counts(normalize = True)) * 100

(lead_df['I agree to pay the amount through cheque'].value_counts(normalize = True)) * 100
# need to drop

(lead_df['A free copy of Mastering The Interview'].value_counts(normalize = True)) * 100

(lead_df['Last Notable Activity'].value_counts(normalize = True)) * 100

# Here we are droppig features which has imbalance data (i.e)., which is of above 70%

lead_df = lead_df.drop(['I agree to pay the amount through cheque','Get updates on DM Content','Update me on Supply Chain Content',
                      'Receive More Updates About Our Courses','Through Recommendations','Digital Advertisement','Newspaper',
                      'X Education Forums','Newspaper Article','Magazine','Search','What matters most to you in choosing a course',
                      'What is your current occupation','Country','Do Not Call','Do Not Email'],axis=1)

lead_df.head()

lead_df.shape

# checking for null values

lead_df.isna().sum()

lead_df.info()

lead_df.describe()

"""## Analyzing Numerical Values"""

# number of null values

lead_df["TotalVisits"].isna().sum()

sns.distplot(lead_df["TotalVisits"])
plt.show()

print("Skewness =", lead_df["TotalVisits"].skew())
print("Kurtosis =", lead_df["TotalVisits"].kurt())

"""* Mean imputation works better if the distribution is normally-distributed or has a Gaussian distribution.
* While median imputation is preferable for skewed data(be it right or left).
"""

# filling null values with median

lead_df["TotalVisits"] = lead_df["TotalVisits"].fillna(lead_df["TotalVisits"].median())
lead_df["TotalVisits"] = lead_df["TotalVisits"].astype(int)

sns.distplot(lead_df["Page Views Per Visit"])
plt.show()

print("Skewness = ", lead_df["Page Views Per Visit"].skew())
print("Kurtosis = ", lead_df["Page Views Per Visit"].kurt())

"""* Here also it is positively skewed.
* We can replace null values with median.
"""

# filling null values with median

lead_df["Page Views Per Visit"] = lead_df["Page Views Per Visit"].fillna(lead_df["Page Views Per Visit"].median())

lead_df.isna().sum()

# all the null values are handled

"""## Finding Outlier"""

plt.boxplot(x=lead_df['TotalVisits'])
plt.show()

# calculating IQR and threshold

IQR1 = np.percentile(lead_df['TotalVisits'],75)-np.percentile(lead_df['TotalVisits'],25)

upper_threshold = np.percentile(lead_df['TotalVisits'],75)+1.5*IQR1
lower_threshold = np.percentile(lead_df['TotalVisits'],25)-1.5*IQR1

print("upper_threshold :",upper_threshold)
print("lower_threshold :",lower_threshold)


upper1 = np.where([lead_df['TotalVisits']>upper_threshold])

lower1 = np.where([lead_df['TotalVisits']<lower_threshold])

lead_df = lead_df.drop(lead_df[(lead_df['TotalVisits']>upper_threshold)].index)

plt.boxplot(x=lead_df['Total Time Spent on Website'])
plt.show()

plt.boxplot(x=lead_df['Page Views Per Visit'])
plt.show()

# calculating IQR and threshold

IQR3 = np.percentile(lead_df['Page Views Per Visit'],75)-np.percentile(lead_df['Page Views Per Visit'],25)

upper_threshold=np.percentile(lead_df['Page Views Per Visit'],75)+1.5*IQR3
lower_threshold=np.percentile(lead_df['Page Views Per Visit'],25)-1.5*IQR3

print("upper_threshold :",upper_threshold)
print("lower_threshold :",lower_threshold)


upper3=np.where([lead_df['Page Views Per Visit']>upper_threshold])

lower3=np.where([lead_df['Page Views Per Visit']<lower_threshold])

lead_df = lead_df.drop(lead_df[(lead_df['Page Views Per Visit']>upper_threshold)].index)

"""* As all outliers are deleted and null values are replaced, now the data is clean."""

lead_df.shape

lead_df.isna().sum()

lead_df.info()

corr = lead_df.corr()
plt.figure(figsize=(13,7))
sns.heatmap(corr, annot = True, cmap = "RdYlGn")
plt.show()

# creating a list of categorical columns

cat_features = ['Lead Origin','Lead Source','Last Activity','Specialization','Tags','City','A free copy of Mastering The Interview','Last Notable Activity']

# fetching columns which have Unknown as value

for i in cat_features:
  print(i)
  print(lead_df[i].isin(["Unknown"]).sum())
  print()

# creating dummy variables

lead_origin = pd.get_dummies(lead_df["Lead Origin"], prefix = "Lead Origin", drop_first = True)
lead_source = pd.get_dummies(lead_df["Lead Source"], prefix = "Lead Source", drop_first = True)
last_activity = pd.get_dummies(lead_df["Last Activity"], prefix = "Last Activity", drop_first = True)
specialization = pd.get_dummies(lead_df["Specialization"], prefix = "Specialization")
tags = pd.get_dummies(lead_df["Tags"], prefix = "Tags", drop_first = True)
city = pd.get_dummies(lead_df["City"], prefix = "City")
free_copy = pd.get_dummies(lead_df["A free copy of Mastering The Interview"], prefix = "A free copy of Mastering The Interview", drop_first = True)
notable_activity = pd.get_dummies(lead_df["Last Notable Activity"], prefix = "Last Notable Activity", drop_first = True)

# creating a copy of the dataframe

lead_df3 = lead_df

# concatenating main dataframe with dummies

lead_df = pd.concat([lead_df, lead_origin, lead_source, last_activity, specialization, tags, city, free_copy, notable_activity], axis = 1)

lead_df.shape

# dropping categorical columns for which dummy were created

lead_df = lead_df.drop(cat_features, axis = 1)

# dropping the columns which have Unknown as value

lead_df = lead_df.drop(["Specialization_Unknown", "City_Unknown"], axis = 1)

lead_df.shape

lead_df.head()

# independent variables

X = lead_df.drop('Converted',axis=1)

# target variable

y = lead_df['Converted']

# splitting data into training and test

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# scaling data

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test=sc.transform(X_test)

# Fitting Logistic Regression to the training set
  
classifier = LogisticRegression(max_iter = 200)  
model = classifier.fit(X_train, y_train)

# using created model to predict outcomes of X_test

y_pred = model.predict(X_test)

# probabilities of X_test data

y_probability = model.predict_proba(X_test)

y_probability = np.round(y_probability, 2) * 100

y_probability[:10]

# plotting confusion matrix

cm = confusion_matrix(y_test, y_pred)
from mlxtend.plotting import plot_confusion_matrix
plot_confusion_matrix(conf_mat=cm, figsize=(6, 6), cmap=plt.cm.Greens)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actual', fontsize=18)
plt.title('Confusion Matrix', fontsize=18)
plt.show()

# accuracy score

accuracy_score(y_pred,y_test)

"""* **The accuracy of the model is 89.49%**

## Finding out the coefficient of each feature so that we can find important features
"""

Coeficient = pd.DataFrame(data={
    'Column Name': list(X.columns),
    'Coefficient': model.coef_[0]
})
Coeficient = Coeficient.sort_values(by='Coefficient', ascending = False)

Coeficient.head(20)

"""Q1. Which are the top three variables in your model which contribute most towards the probability of a lead getting converted?"""

top_3 = {'Tags':1.92, 'Total Time Spent on Website':1.12, 'Last Notable Activity':0.70,}
Attributes = list(top_3.keys())
Importance = list(top_3.values())

plt.figure(figsize = (10, 5))
plt.bar(Attributes, Importance, width = 0.4)
plt.xlabel("Attributes")
plt.ylabel("Coefficient")
plt.title("Top three Important Features")
plt.show()

"""Q2) What are the top three categorical/dummy variables in the model which should be focused the most on in order to increase the probability of lead conversion?"""

top_3 = {'Tags':1.92, 'Last Notable Activity':0.70, 'Lead Origin':0.69}
Attributes = list(top_3.keys())
Importance = list(top_3.values())

plt.figure(figsize = (10, 5))
plt.bar(Attributes, Importance, width = 0.4)
plt.xlabel("Attributes")
plt.ylabel("Coefficient")
plt.title("Top three Important Features")
plt.show()